{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import kornia\n",
    "import utils.definitions as D\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(D.IMAGE_SIZE, D.IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize(size=(D.IMAGE_SIZE, D.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=train_tf)\n",
    "trainloader = DataLoader(trainset, batch_size=D.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "testset = CIFAR10(root='./data', train=False, download=True, transform=test_tf)\n",
    "testloader = DataLoader(testset, batch_size=D.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Testing samples: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CelebA\n",
    "\n",
    "trainset = CelebA(root='./data', split='train', download=True, transform=train_tf)\n",
    "trainloader = DataLoader(trainset, batch_size=D.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "testset = CelebA(root='./data', split='test', download=True, transform=test_tf)\n",
    "testloader = DataLoader(testset, batch_size=D.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Testing samples: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Patches\n",
    "Create patches from input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches():\n",
    "    def __init__(self, patch_size=D.PATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def __call__(self, images):\n",
    "        patches = kornia.contrib.extract_tensor_patches(\n",
    "            input=images,\n",
    "            window_size=(self.patch_size, self.patch_size),\n",
    "            stride=(self.patch_size, self.patch_size),\n",
    "        )\n",
    "        \n",
    "        # (batch, num_patches, channels, patch_height, patch_width)\n",
    "        return patches\n",
    "        \n",
    "    def show_patched_image(self, images, patches):\n",
    "        \"\"\"This is a utility function which accepts a batch of images and its corresponding patches and help visualize one image and its patches side by side.\"\"\"\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        print(f\"Index selected: {idx}.\")\n",
    "\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(images[idx].permute(1, 2, 0).numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        n = int(np.sqrt(patches.shape[1]))\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        for i, patch in enumerate(patches[idx]):\n",
    "            ax = plt.subplot(n, n, i + 1)\n",
    "            patch_img = patch.permute(1, 2, 0).numpy()\n",
    "            plt.imshow(patch_img)\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Return the index chosen to validate it outside the method.\n",
    "        return idx\n",
    "    \n",
    "    def reconstruct_from_patch(self, patch):\n",
    "        \"\"\"Takes the patches from a single image and reconstructs it back into the image\"\"\"\n",
    "        num_patches = patch.shape[0]\n",
    "        nrows = int(np.sqrt(num_patches))\n",
    "        # View as (batch, patch_height, patch_width, channels)\n",
    "        patch = patch.permute(0, 2, 3, 1)\n",
    "        rows = patch.split(nrows, dim=0)\n",
    "        rows = [torch.cat(torch.unbind(x), dim=1) for x in rows]\n",
    "        reconstructed = torch.cat(rows, dim=0)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images.\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Define the patch layer.\n",
    "patch_layer = Patches()\n",
    "\n",
    "# Get the patches from the batched images.\n",
    "patches = patch_layer(images=images)\n",
    "\n",
    "# Pass the images and its patches to the `show_patched_image` method.\n",
    "random_index = patch_layer.show_patched_image(images=images, patches=patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same image and try reconstructing the patches into the original image.\n",
    "image = patch_layer.reconstruct_from_patch(patches[random_index])\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Encoder\n",
    "This layer deals with encoding the patches and adding the positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=D.PATCH_SIZE,\n",
    "        num_patches=D.NUM_PATCHES,\n",
    "        projection_dim=D.ENC_PROJECTION_DIM,\n",
    "        mask_proportion=D.MASK_PROPORTION,\n",
    "        n_channels=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.mask_proportion = mask_proportion\n",
    "        self.num_mask = int(self.mask_proportion * self.num_patches)\n",
    "        \n",
    "        # Create the projection layer for the patches\n",
    "        in_features = n_channels * patch_size * patch_size\n",
    "        self.projection = nn.Linear(in_features=in_features, out_features=self.projection_dim)\n",
    "        \n",
    "        # Create the positional embedding layer\n",
    "        self.positional_embedding = nn.Embedding(num_embeddings=self.num_patches, embedding_dim=self.projection_dim)\n",
    "    \n",
    "    def __call__(self, patches):\n",
    "        # View patches as (batch, num_patches, patch_area)\n",
    "        patches = patches.view(*patches.shape[:2], -1)\n",
    "\n",
    "        # Get the positional embeddings\n",
    "        batch_size = patches.shape[0]\n",
    "        positions = torch.arange(start=0, end=self.num_patches, step=1)\n",
    "        pos_embeddings = self.positional_embedding(positions[None, :].to(device))\n",
    "        pos_embeddings = torch.tile(pos_embeddings, (batch_size, 1, 1))\n",
    "\n",
    "        # Embed the patches\n",
    "        patch_embeddings = (self.projection(patches.to(device)) + pos_embeddings)\n",
    "        \n",
    "        batch_size = patches.shape[0]\n",
    "        mask_indices, unmask_indices = self.get_random_indices(batch_size)\n",
    "\n",
    "        # The encoder input is the unmasked patch embeddings. Here we gather all the patches that should be unmasked.\n",
    "        unmasked_embeddings = self.gather_torch(patch_embeddings, unmask_indices.unsqueeze(2))\n",
    "\n",
    "        # Get the unmasked and masked position embeddings. We will need them for the decoder.\n",
    "        unmasked_positions = self.gather_torch(pos_embeddings, unmask_indices.unsqueeze(2))\n",
    "        masked_positions = self.gather_torch(pos_embeddings, mask_indices.unsqueeze(2))\n",
    "\n",
    "        # Repeat the mask token number of mask times. Mask tokens replace the masks of the image.\n",
    "        mask_token = torch.normal(size=(1, self.patch_size * self.patch_size * 3), mean=0.0, std=1.0)\n",
    "        mask_tokens = mask_token.repeat((self.num_mask, 1))\n",
    "        mask_tokens = mask_tokens.unsqueeze(0).repeat((batch_size, 1, 1))\n",
    "\n",
    "        masked_embeddings = self.projection(mask_tokens.to(device)) + masked_positions\n",
    "\n",
    "        return (\n",
    "            unmasked_embeddings,    # input to the encoder\n",
    "            masked_embeddings,      # first part of input to the decoder\n",
    "            unmasked_positions,     # added to the encoder outputs\n",
    "            mask_indices,           # the indices that were masked\n",
    "            unmask_indices,         # the indices that were unmasked\n",
    "        )\n",
    "    \n",
    "    def get_random_indices(self, batch_size):\n",
    "        \"\"\"Create random indices from a uniform distribution and then split it into mask and unmask indices\"\"\"\n",
    "        rand_indices = torch.argsort(\n",
    "           torch.rand(size=(batch_size, self.num_patches)), dim=-1\n",
    "        )\n",
    "        mask_indices = rand_indices[:, :self.num_mask]\n",
    "        unmask_indices = rand_indices[:, self.num_mask:]\n",
    "        return mask_indices, unmask_indices\n",
    "    \n",
    "    def show_masked_image(self, patches, unmask_indices):\n",
    "        # Choose a random patch and its corresponding unmask index\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        patch = patches[idx]\n",
    "        unmask_index = unmask_indices[idx]\n",
    "\n",
    "        # Build a numpy array of same shape as patch\n",
    "        new_patch = np.zeros_like(patch)\n",
    "\n",
    "        # Iterate over the new_patch and plug the unmasked patches\n",
    "        for i in range(unmask_index.shape[0]):\n",
    "            new_patch[unmask_index[i]] = patch[unmask_index[i]]\n",
    "        return new_patch, idx\n",
    "\n",
    "    def gather_torch(self, params, indices, batch_dim=1):\n",
    "        \"\"\"A PyTorch porting of tensorflow.gather_nd by Kulbear https://gist.github.com/Kulbear/b421c32fb64deddfd0403340db4eeeaa\n",
    "        \n",
    "        Args:\n",
    "            params: a tensor of dimension [b1, ..., bn, g1, ..., gm, c].\n",
    "            indices: a tensor of dimension [b1, ..., bn, x, m]\n",
    "            batch_dim: indicate how many batch dimension you have, in the above example, batch_dim = n.\n",
    "        Returns:\n",
    "            gathered: a tensor of dimension [b1, ..., bn, x, c].\n",
    "        \"\"\"\n",
    "        batch_dims = params.size()[:batch_dim]          # [b1, ..., bn]\n",
    "        batch_size = np.cumprod(list(batch_dims))[-1]   # b1 * ... * bn\n",
    "        c_dim = params.size()[-1]                       # c\n",
    "        grid_dims = params.size()[batch_dim:-1]         # [g1, ..., gm]\n",
    "        n_indices = indices.size(-2)                    # x\n",
    "        n_pos = indices.size(-1)                        # m\n",
    "\n",
    "        # Reshape leading batch dims to a single batch dim\n",
    "        params = params.reshape(batch_size, *grid_dims, c_dim)\n",
    "        indices = indices.reshape(batch_size, n_indices, n_pos)\n",
    "\n",
    "        # Build gather indices\n",
    "        # Gather for each of the data point in this \"batch\"\n",
    "        batch_enumeration = torch.arange(batch_size).unsqueeze(1)\n",
    "        gather_dims = [indices[:, :, i] for i in range(len(grid_dims))]\n",
    "        gather_dims.insert(0, batch_enumeration)\n",
    "        gathered = params[gather_dims]\n",
    "\n",
    "        # Reshape back to the shape with leading batch dims\n",
    "        gathered = gathered.reshape(*batch_dims, n_indices, c_dim)\n",
    "        return gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the patch encoder layer.\n",
    "patch_encoder = PatchEncoder()\n",
    "patch_encoder.to(device)\n",
    "\n",
    "# Get the embeddings and positions.\n",
    "(\n",
    "    unmasked_embeddings,\n",
    "    masked_embeddings,\n",
    "    unmasked_positions,\n",
    "    mask_indices,\n",
    "    unmask_indices,\n",
    ") = patch_encoder(patches=patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a masked patch image.\n",
    "new_patch, random_index = patch_encoder.show_masked_image(patches, unmask_indices)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "img = patch_layer.reconstruct_from_patch(torch.from_numpy(new_patch))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Masked\")\n",
    "plt.subplot(1, 2, 2)\n",
    "img = images[random_index].permute(1, 2, 0)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaskedAutoEncoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm.models.vision_transformer\n",
    "\n",
    "class VisionTransformer(timm.models.vision_transformer.VisionTransformer):\n",
    "    \"\"\" Vision Transformer with support for global average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, global_pool=False, **kwargs):\n",
    "        super(VisionTransformer, self).__init__(**kwargs)\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "        if self.global_pool:\n",
    "            norm_layer = kwargs['norm_layer']\n",
    "            embed_dim = kwargs['embed_dim']\n",
    "            self.fc_norm = norm_layer(embed_dim)\n",
    "\n",
    "            del self.norm  # remove the original norm\n",
    "            \n",
    "    def __call__(self, input):\n",
    "        # Layer normalization\n",
    "        input = self.norm(input)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            input = blk(input)\n",
    "        \n",
    "        # Normalize\n",
    "        input = self.norm(input)\n",
    "        \n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import Block\n",
    "\n",
    "class MAEDecoder(nn.Module):\n",
    "    \"\"\"Decoder for the Masked Autoencoder model [0].\n",
    "\n",
    "    Decodes encoded patches and predicts pixel values for every patch.\n",
    "    Code inspired by [1].\n",
    "\n",
    "    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377\n",
    "    - [1]: https://github.com/facebookresearch/mae\n",
    "\n",
    "    Attributes:\n",
    "        num_patches:\n",
    "            Number of patches.\n",
    "        patch_size:\n",
    "            Patch size.\n",
    "        in_chans:\n",
    "            Number of image input channels.\n",
    "        embed_dim:\n",
    "            Embedding dimension of the encoder.\n",
    "        decoder_embed_dim:\n",
    "            Embedding dimension of the decoder.\n",
    "        decoder_depth:\n",
    "            Depth of transformer.\n",
    "        decoder_num_heads:\n",
    "            Number of attention heads.\n",
    "        mlp_ratio:\n",
    "            Ratio of mlp hidden dim to embedding dim.\n",
    "        proj_drop_rate:\n",
    "            Percentage of elements set to zero after the MLP in the transformer.\n",
    "        attn_drop_rate:\n",
    "            Percentage of elements set to zero after the attention head.\n",
    "        norm_layer:\n",
    "            Normalization layer.\n",
    "        mask_token:\n",
    "            The mask token.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        patch_size,\n",
    "        in_chans=3,\n",
    "        embed_dim=1024,\n",
    "        decoder_embed_dim=512,\n",
    "        decoder_depth=8,\n",
    "        decoder_num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        proj_drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches, decoder_embed_dim), requires_grad=False\n",
    "        )  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(\n",
    "                    decoder_embed_dim,\n",
    "                    decoder_num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    norm_layer=norm_layer,\n",
    "                    proj_drop=proj_drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                )\n",
    "                for i in range(decoder_depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(\n",
    "            decoder_embed_dim, patch_size**2 * in_chans, bias=True\n",
    "        )  # decoder to patch\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Returns predicted pixel values from encoded tokens.\n",
    "\n",
    "        Args:\n",
    "            input:\n",
    "                Tensor with shape (batch_size, seq_length, embed_input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, seq_length, out_dim).\n",
    "\n",
    "        \"\"\"\n",
    "        out = self.embed(input)\n",
    "        out = self.decode(out)\n",
    "        return self.predict(out)\n",
    "\n",
    "    def embed(self, input):\n",
    "        \"\"\"Embeds encoded input tokens into decoder token dimension.\n",
    "\n",
    "        This is a single linear layer that changes the token dimension from\n",
    "        embed_input_dim to hidden_dim.\n",
    "\n",
    "        Args:\n",
    "            input:\n",
    "                Tensor with shape (batch_size, seq_length, embed_input_dim)\n",
    "                containing the encoded tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, seq_length, hidden_dim) containing\n",
    "            the embedded tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        out = self.decoder_embed(input)\n",
    "        return out\n",
    "\n",
    "    def decode(self, input):\n",
    "        \"\"\"Forward pass through the decoder transformer.\n",
    "\n",
    "        Args:\n",
    "            input:\n",
    "                Tensor with shape (batch_size, seq_length, hidden_dim) containing\n",
    "                the encoded tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, seq_length, hidden_dim) containing\n",
    "            the decoded tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.decoder_blocks(input)\n",
    "        output = self.decoder_norm(output)\n",
    "        return output\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\"Predics pixel values from decoded tokens.\n",
    "\n",
    "        Args:\n",
    "            input:\n",
    "                Tensor with shape (batch_size, seq_length, hidden_dim) containing\n",
    "                the decoded tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, seq_length, out_dim) containing\n",
    "            predictions for each token.\n",
    "\n",
    "        \"\"\"\n",
    "        out = self.decoder_pred(input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VisionTransformer(\n",
    "    patch_size=D.PATCH_SIZE,\n",
    "    embed_dim=D.ENC_PROJECTION_DIM,\n",
    "    depth=D.ENC_LAYERS,\n",
    "    num_heads=D.ENC_NUM_HEADS,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    norm_layer=partial(nn.LayerNorm, eps=D.LAYER_NORM_EPS),\n",
    ")\n",
    "encoder.to(device)\n",
    "\n",
    "decoder = MAEDecoder(\n",
    "    num_patches=D.NUM_PATCHES,\n",
    "    patch_size=D.PATCH_SIZE,\n",
    "    embed_dim=D.ENC_PROJECTION_DIM,\n",
    "    decoder_embed_dim=D.DEC_PROJECTION_DIM,\n",
    "    decoder_depth=D.DEC_LAYERS,\n",
    "    decoder_num_heads=D.DEC_NUM_HEADS,\n",
    "    mlp_ratio=4.0,\n",
    "    proj_drop_rate=0.1,\n",
    "    attn_drop_rate=0.1,\n",
    ")\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaskedAutoEncoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_layer,\n",
    "        patch_encoder,\n",
    "        encoder,\n",
    "        decoder,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = D.MASK_PROPORTION\n",
    "        self.patch_layer = patch_layer\n",
    "        self.patch_encoder = patch_encoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward_encoder(self, unmasked_embeddings):\n",
    "        return self.encoder(unmasked_embeddings)\n",
    "\n",
    "    def forward_decoder(self, decoder_inputs):\n",
    "        # Decoder forward pass\n",
    "        x_decode = self.decoder.embed(decoder_inputs)\n",
    "        x_decoded = self.decoder.decode(x_decode)\n",
    "        # Predict pixel values\n",
    "        x_pred = self.decoder.predict(x_decoded)\n",
    "        return x_pred\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Patch the images\n",
    "        patches = self.patch_layer(images)\n",
    "        \n",
    "        # Reshape patches as (batch_size, num_patches, channels*patch_size**2)\n",
    "        patches = patches.reshape(*patches.size()[:2], -1)\n",
    "        \n",
    "        # Encode the patches.\n",
    "        (\n",
    "            unmasked_embeddings,\n",
    "            masked_embeddings,\n",
    "            unmasked_positions,\n",
    "            mask_indices,\n",
    "            unmask_indices,\n",
    "        ) = self.patch_encoder(patches)\n",
    "\n",
    "        # Pass the unmasked patches to the encoder\n",
    "        encoder_outputs = self.forward_encoder(unmasked_embeddings)\n",
    "        \n",
    "        # Create the decoder inputs\n",
    "        encoder_outputs = encoder_outputs + unmasked_positions\n",
    "        decoder_inputs = torch.concat((encoder_outputs, masked_embeddings), dim=1)\n",
    "        \n",
    "        # Decode the inputs\n",
    "        decoder_outputs = self.forward_decoder(decoder_inputs)\n",
    "        \n",
    "        predictions = self.patch_encoder.gather_torch(decoder_outputs, mask_indices.unsqueeze(2))\n",
    "        targets = self.patch_encoder.gather_torch(patches, mask_indices.unsqueeze(2))\n",
    "\n",
    "        return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_model = MaskedAutoEncoder(\n",
    "    patch_layer=patch_layer,\n",
    "    patch_encoder=patch_encoder,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")\n",
    "mae_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(mae_model.parameters(), lr=D.LEARNING_RATE, weight_decay=D.WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run\n",
    "predictions, targets = mae_model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the model's progress\n",
    "class TrainMonitor():\n",
    "    def __init__(self, model, testloader, epoch_interval=5):\n",
    "        self.epoch_interval = epoch_interval\n",
    "        self.model = model\n",
    "        self.test_images, _ = next(iter(testloader))\n",
    "\n",
    "    def output_progress(self, epoch):\n",
    "        if epoch % self.epoch_interval != 0:\n",
    "            return\n",
    "        \n",
    "        test_patches = self.model.patch_layer(self.test_images)\n",
    "        (\n",
    "            test_unmasked_embeddings,\n",
    "            test_masked_embeddings,\n",
    "            test_unmasked_positions,\n",
    "            test_mask_indices,\n",
    "            test_unmask_indices,\n",
    "        ) = self.model.patch_encoder(test_patches)\n",
    "        \n",
    "        test_encoder_outputs = self.model.forward_encoder(test_unmasked_embeddings)\n",
    "        test_encoder_outputs = test_encoder_outputs + test_unmasked_positions\n",
    "        \n",
    "        test_decoder_inputs = torch.concat((test_encoder_outputs, test_masked_embeddings), dim=1)\n",
    "        \n",
    "        # (batch_size, image_h, image_w, channels)\n",
    "        test_decoder_outputs = self.model.forward_decoder(test_decoder_inputs)\n",
    "\n",
    "        # Show a masked patch image.\n",
    "        test_masked_patch, idx = self.model.patch_encoder.show_masked_image(\n",
    "            test_patches, test_unmask_indices\n",
    "        )\n",
    "        print(test_masked_patch.shape)\n",
    "        print(f\"\\nIdx chosen: {idx}\")\n",
    "        original_image = self.test_images[idx]\n",
    "        masked_image = self.model.patch_layer.reconstruct_from_patch(\n",
    "            torch.from_numpy(test_masked_patch)\n",
    "        )\n",
    "        \n",
    "        # (image_h, image_w, channels)\n",
    "        reconstructed_image = test_decoder_outputs[idx]\n",
    "        print(test_decoder_outputs.shape)\n",
    "        print(reconstructed_image.shape)\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "        ax[0].imshow(original_image.permute(1, 2, 0))\n",
    "        ax[0].set_title(f\"Original: {epoch:03d}\")\n",
    "\n",
    "        ax[1].imshow(masked_image)\n",
    "        ax[1].set_title(f\"Masked: {epoch:03d}\")\n",
    "\n",
    "        ax[2].imshow(reconstructed_image.cpu().detach().numpy())\n",
    "        ax[2].set_title(f\"Resonstructed: {epoch:03d}\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training\")\n",
    "num_epochs = D.EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    train_loop = tqdm(trainloader, leave=False)\n",
    "    for images, labels in train_loop:\n",
    "        images = images.to(device)\n",
    "        predictions, targets = mae_model(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loop.set_description(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    avg_loss = total_loss / len(trainloader)\n",
    "    print(f\"train_loss: {avg_loss:.5f}\")\n",
    "    train_monitor = TrainMonitor(mae_model, testloader)\n",
    "    train_monitor.output_progress(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
